facet_wrap(~ topic, scales = "free") +
coord_flip()
hm_tfidf <- DocumentTermMatrix(hm_corpus, control = list(weighting = weightTfIdf))
hm_lda_2 <- LDA(hm_tfidf, k = 2, control = list(seed = 1234))
text_topics
text_topics$term=="the"
sum(text_topics$term=="the")
data
#1.
data=ToothGrowth
data
OJ_len = data[data$supp=="OJ",3]
OJ_len
OJ_len = data[data$supp=="OJ",1]
OJ_len
VC_len = data[data$supp=="VC",1]
VC_len
#(a)A parametric procedure
#T-test. H0: mu_o = mu_b,  H1: mu_o != mu_b
t.test(OJ_len,VC_len)
#(b)A non-parametric procedure
#Wilcoxon rank sum test. H0: mu_o - mu_b = 0,  H1: mu_o - mu_b != 0
wilcox.test(OJ_len,VC_len,paired=TRUE)
#1.
data=ToothGrowth
OJ_len = data[data$supp=="OJ",1]
VC_len = data[data$supp=="VC",1]
#(a)A parametric procedure
#T-test. H0: mu_o = mu_b,  H1: mu_o != mu_b
t.test(OJ_len,VC_len)
#(b)A non-parametric procedure
#Wilcoxon rank sum test. H0: mu_o - mu_b = 0,  H1: mu_o - mu_b != 0
wilcox.test(OJ_len,VC_len,paired=TRUE)
data
qqplot<-ggplot(data,aes(sample=len))
#Normality check
library(ggplot2)
qqplot<-ggplot(data,aes(sample=len))
qqplot+stat_qq(aes(color=supp))
ggplot(data=ToothGrowth)+
geom_histogram(aes(data$len),fill=supp)
ggplot(data=ToothGrowth)+
geom_histogram(aes(data$len,fill=supp))
ggplot(data=ToothGrowth)+
geom_histogram(aes(data$len))+
facet_wrap(~supp)
ggplot(data=ToothGrowth)+
geom_histogram(aes(data$len))+
facet_wrap(~supp)
ggplot(data=ToothGrowth)+
geom_histogram(aes(data$len))+
facet_wrap(~supp,nrow=2)
ggplot(data=ToothGrowth)+
geom_histogram(aes(data$len))+
facet_wrap(~supp,nrow=2)
boxplot(VC_len,OJ_len,names = c("VC","OJ"),ylab="tooth length")
boxplot(VC_len,OJ_len,names = c("VC","OJ"),ylab="tooth length")
cor.test(VC_len,OJ_len)
plot(VC_len,OJ_len)
length(VC_len)
vc.median.boot <- boot(EDA_VC$len,statistic = function(d,ind){median(d[ind])},R = 1000,sim = "ordinary")
OJ.median.boot <- boot(EDA_OJ$len,statistic = function(d,ind){median(d[ind])},R = 1000,sim = "ordinary")
#1)A 95% confidence interval for the difference in median weight for the two groups
library(boot)
set.seed(0)
vc.median.boot <- boot(EDA_VC$len,statistic = function(d,ind){median(d[ind])},R = 1000,sim = "ordinary")
set.seed(0)
vc.median.boot <- boot(VC$len,statistic = function(d,ind){median(d[ind])},R = 1000,sim = "ordinary")
OJ.median.boot <- boot(OJ$len,statistic = function(d,ind){median(d[ind])},R = 1000,sim = "ordinary")
vc.median.boot <- boot(VC_len,statistic = function(d,ind){median(d[ind])},R = 1000,sim = "ordinary")
OJ.median.boot <- boot(OJ_len,statistic = function(d,ind){median(d[ind])},R = 1000,sim = "ordinary")
#For OJ
boot.ci(OJ.median.boot, conf = 0.95, type = c("norm","basic","perc","bca"))
#For VC
boot.ci(vc.median.boot, conf = 0.95, type = c("norm","basic","perc","bca"))
#For OJ
boot.ci(OJ.median.boot-vc.median.boot, conf = 0.95, type = c("norm","basic","perc","bca"))
vC-OJ.median.boot <- boot(VC_len-OJ_len,statistic = function(d,ind){median(d[ind])},R = 1000,sim = "ordinary")
vC_diff_OJ.median.boot <- boot(VC_len-OJ_len,statistic = function(d,ind){median(d[ind])},R = 1000,sim = "ordinary")
boot.ci(vC_diff_OJ.median.boot, conf = 0.95, type = c("norm","basic","perc","bca"))
len_df <- data.frame(VC=VC_len,OJ=OJ_len)
bootratios <- do(10000) * varianceratio(VC~OJ, data=resample(len_df))
varianceratio(VC~OJ, data=resample(len_df))
varianceratio <- function (x, ..., data = parent.frame(), only.2 = TRUE)
{
v <- var(x, ..., data = data)
res <- v/lag(v)  # This calculates the 2nd variance / 1st variance
res[2]
}
varianceratio(VC~OJ, data=resample(len_df))
varianceratio(VC~OJ, data=len_df)
#b)A 95% bootstrap confidence interval for the ratio of the variances of the two groups
varianceratio <- function (x, ..., data1 = parent.frame(), only.2 = TRUE)
{
v <- var(x, ..., data = data1)
res <- v/lag(v)  # This calculates the 2nd variance / 1st variance
res[2]
}
varianceratio(VC~OJ, data=resample(len_df))
#b)A 95% bootstrap confidence interval for the ratio of the variances of the two groups
vC_var.boot <- boot(VC_len,statistic = function(d,ind){var(d[ind])},R = 1000,sim = "ordinary")
var(VC_len)
vC_var.boot
vC_var.boot$t
OJ_var.boot <- boot(OJ_len,statistic = function(d,ind){var(d[ind])},R = 1000,sim = "ordinary")
boot.ci(vC_var.boot/OJ_var.boot, conf = 0.95, type = c("norm","basic","perc","bca"))
confint(vC_var.boot$t/OJ_var.boot$t, level = 0.95, method = "all")
vC_var.boot$t/OJ_var.boot$t
var_ratio <- vC_var.boot$t/OJ_var.boot$t
confint(var_ratio, level = 0.95, method = "all")
confint(vC_var.boot$t, level = 0.95, method = "all")
var_ratio <- c(vC_var.boot$t/OJ_var.boot$t)
var_ratio
confint(var_ratio, level = 0.95, method = "all")
?confit
confint(VC_len, level = 0.95, method = "all")
library(stats)
?confint
confint(var_ratio, level = 0.95, method = "all")
vC_var.boot/OJ_var.boot
#Bootstrap quantile method
quantile(median(var_ratio), c(0.025, 0.975))
vC_diff_OJ.median.boot
var(VC_len/OJ_len)
var(VC_len)/var(OJ_len)
var_ratio
# Set seed for random number generator
set.seed(3141)
# Generate 28 trained and 22 untrained teachers
group <- c(rep("trained", 28), rep("untrained", 22))
# Generate essay scores for each group from a normal distribution
# Trained scores ~ normal(mean = 4.5, std. dev = 2.1)
# Untrained scores ~ normal(mean = 7.3, std. dev = 4.2)
# I round scores to the nearest integer (0 decimal places)
score <- c(round(rnorm(28, 4.5, 2.1),0), round(rnorm(22, 7.3, 4.2),0))
# Combine the group and score variables into the same data frame
essay <- data.frame(group, score)
densityplot(~score | group, data=essay, lw=4, bw=2, col="steelblue", layout=c(1,2), cex=1.1)
favstats(score ~ group, data=essay)
#b)A 95% bootstrap confidence interval for the ratio of the variances of the two groups
library(tsub)
install.packages("tsub")
install.packages("mosaic")
#b)A 95% bootstrap confidence interval for the ratio of the variances of the two groups
library(tsub)
library(mosaic)
library(mosaic)
set.seed(3141)
# Generate 28 trained and 22 untrained teachers
group <- c(rep("trained", 28), rep("untrained", 22))
# Generate essay scores for each group from a normal distribution
# Trained scores ~ normal(mean = 4.5, std. dev = 2.1)
# Untrained scores ~ normal(mean = 7.3, std. dev = 4.2)
# I round scores to the nearest integer (0 decimal places)
score <- c(round(rnorm(28, 4.5, 2.1),0), round(rnorm(22, 7.3, 4.2),0))
# Combine the group and score variables into the same data frame
essay <- data.frame(group, score)
densityplot(~score | group, data=essay, lw=4, bw=2, col="steelblue", layout=c(1,2), cex=1.1)
favstats(score ~ group, data=essay)
varianceratio <- function (x, ..., data = parent.frame(), only.2 = TRUE)
{
v <- var(x, ..., data = data)
res <- v/lag(v)  # This calculates the 2nd variance / 1st variance
res[2]
}
# Test this function on our dataset
varianceratio(score~group, data=essay)
ratios <- do(10000) * varianceratio(score ~ shuffle(group), data=essay)
essay
data
bootratios <- do(10000) * varianceratio(len ~ supp, data=resample(data))
confint(bootratios, level = 0.95, method = "all")
?confint
mosaic.confint(bootratios, level = 0.95, method = c('percentile','bootstrap-t','basic'))
mosai::confint(bootratios, level = 0.95, method = c('percentile','bootstrap-t','basic'))
mosaic::confint(bootratios, level = 0.95, method = c('percentile','bootstrap-t','basic'))
confint(bootratios, level = 0.95, method = c('percentile','bootstrap-t','basic'))
confint(bootratios, level = 0.95, method = c('percentile',"stderr", "bootstrap-t",'basic'))
#Ignore ‘dose’, and determine whether there is a significant difference in the proportions of the two groups  classified as “HIGH” using a suitable test and  a 95% confidence interval.
data[data$len>20,4]=='High'
data
#Ignore ‘dose’, and determine whether there is a significant difference in the proportions of the two groups  classified as “HIGH” using a suitable test and  a 95% confidence interval.
data[data$len>20,4]='High'
data
data[data$len<=20,4]='Low'
#1.
data=ToothGrowth
#Ignore ‘dose’, and determine whether there is a significant difference in the proportions of the two groups  classified as “HIGH” using a suitable test and  a 95% confidence interval.
Status=c(rep(NA,60))
cbind(data,Status)
data
Status=c(rep(NA,60))
data=cbind(data,Status)
data
data[data$len>20,4]='High'
data[data$len<=20,4]='Low'
data
#(a)A parametric procedure
#T-test. H0: mu_o = mu_b,  H1: mu_o != mu_b
t.test(OJ_len,VC_len)
#H0: mu_H - mu_L = 0,  H1: mu_H - mu_L != 0
High_len=data[data$Status=='High',1]
High_len
Low_len=data[data$Status=='Low',1]
t.test(High_len,Low_len)
data
setwd("C:/Users/65451_000/Desktop/Columbia/AU 18/5243/Project1-RNotebook/output")
library(tidyverse)
library(tidytext)
library(DT)
library(scales)
library(wordcloud2)
library(gridExtra)
library(ngram)
library(dplyr)
library(janeaustenr)
library(tidytext)
library(SnowballC)
library(wordcloud)
library(koRpus)
hm_data <- read_csv("../output/processed_moments.csv")
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
hm_data <- hm_data %>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
gender,
marital,
parenthood,
reflection_period,
age,
country,
ground_truth_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
filter(parenthood %in% c("n", "y")) %>%
filter(reflection_period %in% c("24h", "3m")) %>%
mutate(reflection_period = fct_recode(reflection_period,
months_3 = "3m", hours_24 = "24h"))
bag_of_words <-  hm_data %>%
unnest_tokens(word, text)
word_count <- bag_of_words %>%
count(wid,word,gender,marital,parenthood,reflection_period,country, sort = TRUE)
wordcloud(word_count$word, word_count$n, max.words=100, colors=brewer.pal(1, "Dark2"))
wordcloud(word_count$word, word_count$n, max.words=100, colors=brewer.pal(1, "Dark2"))
word_count
word_count_only <- bag_of_words %>%
count(word, sort = TRUE)
wordcloud(word_count_only$word, word_count_only$n, max.words=100, colors=brewer.pal(1, "Dark2"))
wordcloud(word_count_only$word, word_count_only$n, max.words=100, colors=brewer.pal(1, "Dark2"))
library(textstem)
bag_of_words$word=lemmatize_words(bag_of_words$word)
word_count_lemm <- bag_of_words %>%
count(word, sort = TRUE)
library(tm)
hm_corpus <- VCorpus(VectorSource(hm_data$original_hm))
hm_corpus = tm_map(hm_corpus, content_transformer(tolower))
hm_corpus = tm_map(hm_corpus, removeNumbers)
hm_corpus = tm_map(hm_corpus, removePunctuation)
hm_corpus = tm_map(hm_corpus, removeWords, c("the", "and",'day', stopwords("english")))
hm_corpus =  tm_map(hm_corpus, stripWhitespace)
hm_tfidf <- DocumentTermMatrix(hm_corpus, control = list(weighting = weightTfIdf))
hm_tfidf = removeSparseTerms(hm_tfidf, 0.99)
freq = data.frame(sort(colSums(as.matrix(hm_tfidf)), decreasing=TRUE))
df=data.frame(lemm_words=lemmatize_words(rownames(freq)),score=freq[,1])
df_new=df[!duplicated(df$lemm_words), ]
wordcloud(df_new$lemm_words, df_new$score, max.words=100, colors=brewer.pal(3, "Dark2"))
wordcloud(df_new$lemm_words, df_new$score, max.words=100, colors=brewer.pal(3, "Dark2"))
hm_words_time <- word_count %>%
bind_tf_idf(word, wid, n)
hm_words_time
hm_words_time <- hm_words_time[nchar(hm_words_time$word)>3,]
hm_words_time %>%
group_by(reflection_period) %>%
top_n(15, tf_idf) %>%
ungroup() %>%
mutate(word = reorder(word, tf_idf)) %>%
ggplot(aes(word, tf_idf, fill = gender)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~reflection_period, ncol = 2, scales = "free") +
coord_flip()
hm_words_time %>%
group_by(reflection_period) %>%
top_n(15, tf_idf) %>%
ungroup() %>%
mutate(word = reorder(word, tf_idf)) %>%
ggplot(aes(word, tf_idf, fill = gender)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~reflection_period, ncol = 2, scales = "free") +
coord_flip()
young_words <-  hm_data %>%
filter(age >=17 & age<30) %>%
unnest_tokens(word, text)
mid_age_words <- hm_data %>%
filter(age >=30 & age<60) %>%
unnest_tokens(word, text)
senior_words <- hm_data %>%
filter(age >60) %>%
unnest_tokens(word, text)
age_stage <- bind_rows(young_words %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative,method="young_adult"),
mid_age_words %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative,method="mid_aged"),
senior_words %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative,method="senior"))
age_stage %>%
ggplot(aes(word, sentiment, fill = method)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "sentiment")+
facet_wrap(~method, ncol = 1, scales = "free_y")
age_stage %>%
ggplot(aes(word, sentiment, fill = method)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "sentiment")+
facet_wrap(~method, ncol = 1, scales = "free_y")
ggplot(age_stage, aes(x = method, y = sentiment, color = method)) +
geom_boxplot() # draw a boxplot for each president
library(reshape2)
bag_of_words %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray80", "gray20"),
max.words = 100)
bag_of_words %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray80", "gray20"),
max.words = 100)
library(igraph)
library(ggraph)
hm_bigrams <- hm_data %>%
filter(count != 1) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
bigram_counts <- hm_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
count(word1, word2, sort = TRUE)
bigram_graph <- bigram_counts %>%
filter(n > 100) %>%
graph_from_data_frame()
set.seed(0)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
set.seed(0)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
library(topicmodels)
hm_lda_2 <- LDA(DocumentTermMatrix(hm_corpus), k = 2, control = list(seed = 1234))
text_topics <- tidy(hm_lda_2, matrix = "beta")
text_top_terms <- text_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
text_top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
text_top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
beta_spread <- text_topics %>%
mutate(topic = paste0("topic", topic)) %>%
spread(topic, beta) %>%
filter(topic1 > .001 | topic2 > .001) %>%
mutate(log_ratio = log2(topic2 / topic1))
beta_spread %>%
group_by(direction = log_ratio > 0) %>%
top_n(15, abs(log_ratio)) %>%
ungroup() %>%
mutate(term = reorder(term, log_ratio)) %>%
ggplot(aes(term, log_ratio)) +
geom_col() +
labs(y = "Log2 ratio of beta in topic 2 / topic 1") +
coord_flip()
beta_spread %>%
group_by(direction = log_ratio > 0) %>%
top_n(15, abs(log_ratio)) %>%
ungroup() %>%
mutate(term = reorder(term, log_ratio)) %>%
ggplot(aes(term, log_ratio)) +
geom_col() +
labs(y = "Log2 ratio of beta in topic 2 / topic 1") +
coord_flip()
hm_lda_6 <- LDA(DocumentTermMatrix(hm_corpus), k = 6, control = list(seed = 1234))
text_topics1 <- tidy(hm_lda_6, matrix = "beta")
text_top_terms1 <- text_topics1 %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
text_top_terms1 %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
text_top_terms1 %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
hm_lda_6 <- LDA(DocumentTermMatrix(hm_corpus), k = 6, control = list(seed = 1234))
text_topics1 <- tidy(hm_lda_6, matrix = "beta")
text_top_terms1 <- text_topics1 %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
text_top_terms1 %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
hm_corpus <- VCorpus(VectorSource(hm_data$original_hm))
hm_corpus = tm_map(hm_corpus, content_transformer(tolower))
hm_corpus = tm_map(hm_corpus, removeNumbers)
hm_corpus = tm_map(hm_corpus, removePunctuation)
hm_corpus = tm_map(hm_corpus, removeWords, c("the", "and",'day', stopwords("english")))
hm_corpus =  tm_map(hm_corpus, stripWhitespace)
hm_lda_2 <- LDA(DocumentTermMatrix(hm_corpus), k = 2, control = list(seed = 2018))
text_topics <- tidy(hm_lda_2, matrix = "beta")
text_top_terms <- text_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
text_top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
beta_spread <- text_topics %>%
mutate(topic = paste0("topic", topic)) %>%
spread(topic, beta) %>%
filter(topic1 > .001 | topic2 > .001) %>%
mutate(log_ratio = log2(topic2 / topic1))
beta_spread %>%
group_by(direction = log_ratio > 0) %>%
top_n(15, abs(log_ratio)) %>%
ungroup() %>%
mutate(term = reorder(term, log_ratio)) %>%
ggplot(aes(term, log_ratio)) +
geom_col() +
labs(y = "Log2 ratio of beta in topic 2 / topic 1") +
coord_flip()
hm_lda_6 <- LDA(DocumentTermMatrix(hm_corpus), k = 6, control = list(seed = 2018))
hm_lda_6 <- LDA(DocumentTermMatrix(hm_corpus), k = 6, control = list(seed = 2018))
text_topics1 <- tidy(hm_lda_6, matrix = "beta")
text_top_terms1 <- text_topics1 %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
text_top_terms1 %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
text_top_terms1 %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
hm_lda_6 <- LDA(DocumentTermMatrix(hm_corpus), k = 6, control = list(seed = 0))
text_topics1 <- tidy(hm_lda_6, matrix = "beta")
text_top_terms1 <- text_topics1 %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
text_top_terms1 %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
